{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/eshtiaqueahmed/emotion-recognition?scriptVersionId=272931981\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install numpy pandas librosa soundfile scikit-learn matplotlib tensorflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:07:52.128702Z","iopub.execute_input":"2025-09-02T14:07:52.128993Z","iopub.status.idle":"2025-09-02T14:07:55.494653Z","shell.execute_reply.started":"2025-09-02T14:07:52.12897Z","shell.execute_reply":"2025-09-02T14:07:55.493909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport soundfile as sf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nimport matplotlib.pyplot as plt\nimport itertools\n\n\n\n# --------- CONFIG ----------\n#import drive\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n\nDATA_PATH = \"/kaggle/input/ravdess-emotional-speech-audio\"   # <-- set this to where your WAV files are\nSR = 22050                    # sampling rate for librosa.load\nN_MFCC = 40                   # number of MFCC coefficients\nMAX_PAD_LEN = 174             # number of time frames to pad/truncate to (tunable)\nBATCH_SIZE = 32\nEPOCHS = 40\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\nMODEL_OUT = \"ravdess_ser_cnn.h5\"\n# ---------------------------\n\n# RAVDESS emotion mapping (filename 3rd field)\nEMOTIONS = {\n    '01': 'neutral',\n    '02': 'calm',\n    '03': 'happy',\n    '04': 'sad',\n    '05': 'angry',\n    '06': 'fearful',\n    '07': 'disgust',\n    '08': 'surprised'\n}\n\ndef extract_emotion_from_filename(filename):\n    # filename is e.g. .../03-01-05-01-02-01-12.wav\n    base = os.path.basename(filename)\n    parts = base.split('.')[0].split('-')\n    if len(parts) < 3:\n        return None\n    emo_code = parts[2]\n    return EMOTIONS.get(emo_code)\n\ndef pad_or_truncate(mfcc, max_len=MAX_PAD_LEN):\n    # mfcc shape: (n_mfcc, time)\n    if mfcc.shape[1] < max_len:\n        pad_width = max_len - mfcc.shape[1]\n        mfcc = np.pad(mfcc, pad_width=((0,0),(0,pad_width)), mode='constant')\n    else:\n        mfcc = mfcc[:, :max_len]\n    return mfcc\n\ndef extract_features(file_path, sr=SR, n_mfcc=N_MFCC, max_pad_len=MAX_PAD_LEN):\n    # load audio\n    try:\n        y, sr = librosa.load(file_path, sr=sr)\n    except Exception as e:\n        print(f\"Error loading {file_path}: {e}\")\n        return None\n    # compute MFCCs\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n    # compute deltas\n    delta = librosa.feature.delta(mfcc)\n    delta2 = librosa.feature.delta(mfcc, order=2)\n    # stack MFCC + delta + delta2 (resulting in channels = 3)\n    stacked = np.vstack([mfcc, delta, delta2])  # shape (n_mfcc*3, time)\n    # pad / truncate in time dimension\n    stacked = pad_or_truncate(stacked, max_pad_len)\n    return stacked\n\n\n\ndef load_dataset(data_path):\n    pattern = os.path.join(data_path, '**', '*.wav')\n    files = glob.glob(pattern, recursive=True)\n    print(f\"Found {len(files)} wav files.\")\n    X, Y = [], []\n    for f in files:\n        emo = extract_emotion_from_filename(f)\n        if emo is None:\n            continue\n        feat = extract_features(f)\n        if feat is None:\n            continue\n        X.append(feat)\n        Y.append(emo)\n    X = np.array(X)\n    Y = np.array(Y)\n    print(\"X shape (num_samples, features, time):\", X.shape)\n    return X, Y\n\ndef build_cnn_model(input_shape, num_classes):\n    # input_shape: (channels, time) where channels = n_mfcc*3\n    # We'll treat input as \"image\" with shape (channels, time, 1)\n    inp = layers.Input(shape=(input_shape[0], input_shape[1], 1))\n    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inp)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2,2))(x)\n\n    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2,2))(x)\n\n    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2,2))(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.4)(x)\n    out = layers.Dense(num_classes, activation='softmax')(x)\n\n    model = models.Model(inputs=inp, outputs=out)\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix'):\n    if normalize:\n        cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n    plt.figure(figsize=(8,6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        val = f\"{cm[i, j]:.2f}\" if normalize else f\"{int(cm[i, j])}\"\n        plt.text(j, i, val,\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.show()\n\ndef main():\n    X, Y = load_dataset(DATA_PATH)\n\n    # limit classes if you want (example: use only a subset). Here we use all 8 emotions.\n    labels = np.unique(Y)\n    print(\"Emotions in data:\", labels)\n\n    # reshape X for CNN: currently (N, features, time) -> convert to (N, features, time, 1)\n    X = X[..., np.newaxis]\n\n    # encode labels\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(Y)\n    num_classes = len(le.classes_)\n    y_cat = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n\n    # train-test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y_cat, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded\n    )\n\n    print(\"Train shape:\", X_train.shape, y_train.shape)\n    print(\"Test shape:\", X_test.shape, y_test.shape)\n\n    # build model\n    input_shape = (X.shape[1], X.shape[2])  # (features, time)\n    model = build_cnn_model((input_shape[0], input_shape[1]), num_classes)\n    model.summary()\n\n    # callbacks\n    cb = [\n        callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n        callbacks.ModelCheckpoint(MODEL_OUT, save_best_only=True, monitor='val_loss')\n    ]\n\n    history = model.fit(\n        X_train,\n        y_train,\n        validation_split=0.1,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=cb,\n        verbose=1\n    )\n\n    # evaluate\n    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.4f}\")\n\n\n    cnn_result = {'Model': 'CNN', 'Test Accuracy': acc, 'Test Loss': loss}\n\n\n    # predictions and classification report\n    y_pred = model.predict(X_test)\n    y_pred_labels = np.argmax(y_pred, axis=1)\n    y_true_labels = np.argmax(y_test, axis=1)\n\n    print(\"Classification report:\")\n    print(classification_report(y_true_labels, y_pred_labels, target_names=le.classes_))\n\n    # confusion matrix\n    cm = confusion_matrix(y_true_labels, y_pred_labels)\n    plot_confusion_matrix(cm, classes=le.classes_, normalize=False, title='Confusion matrix (counts)')\n    plot_confusion_matrix(cm, classes=le.classes_, normalize=True, title='Confusion matrix (normalized)')\n\n    # plot training history\n    plt.figure(figsize=(12,4))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.legend(); plt.title('Loss')\n    plt.subplot(1,2,2)\n    plt.plot(history.history['accuracy'], label='acc')\n    plt.plot(history.history['val_accuracy'], label='val_acc')\n    plt.legend(); plt.title('Accuracy')\n    plt.show()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:08:30.037166Z","iopub.execute_input":"2025-09-02T16:08:30.037868Z","iopub.status.idle":"2025-09-02T16:10:20.320953Z","shell.execute_reply.started":"2025-09-02T16:08:30.037844Z","shell.execute_reply":"2025-09-02T16:10:20.320103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#----------------------------------------------\n# Experiment 1: LSTM Model Only\n#----------------------------------------------\nprint(\"--- Starting LSTM Model Experiment ---\")\n\n\ndef build_lstm_model(input_shape, num_classes):\n    inp = layers.Input(shape=input_shape)\n    x = layers.LSTM(128, return_sequences=True)(inp)\n    x = layers.Dropout(0.3)(x)\n    x = layers.LSTM(64)(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(64, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    out = layers.Dense(num_classes, activation='softmax')(x)\n    model = models.Model(inputs=inp, outputs=out)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Step 1: Load data and encode labels\nX, Y = load_dataset(DATA_PATH)\nle = LabelEncoder()\ny_encoded = le.fit_transform(Y)\nnum_classes = len(le.classes_)\ny_cat = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n\n# Step 2: Reshape data for LSTM (transpose)\n# Shape change: (samples, features, time) -> (samples, time, features)\nprint(\"Reshaping data for LSTM...\")\nX_for_rnn = np.transpose(X, (0, 2, 1))\nprint(\"New data shape:\", X_for_rnn.shape)\n\n# Step 3: Split data into Train-Test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_for_rnn, y_cat, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded\n)\n\n# Step 4: Build the LSTM model\ninput_shape_lstm = (X_train.shape[1], X_train.shape[2]) # Shape: (time, features)\nmodel_lstm = build_lstm_model(input_shape_lstm, num_classes)\nmodel_lstm.summary()\n\ncb = [\n    callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n    callbacks.ModelCheckpoint(MODEL_OUT, save_best_only=True, monitor='val_loss')\n]\n\n# Step 5: Train the model\nprint(\"\\n--- Starting LSTM Model Training ---\")\nhistory_lstm = model_lstm.fit(\n    X_train, y_train,\n    validation_split=0.1,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=cb,\n    verbose=1\n)\n\n# Step 6: Evaluate the model\nprint(\"\\n--- LSTM Model Evaluation ---\")\nloss, acc = model_lstm.evaluate(X_test, y_test, verbose=0)\nprint(f\"LSTM Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n\nlstm_result = {'Model': 'LSTM', 'Test Accuracy': acc, 'Test Loss': loss}\n\n# Classification report and confusion matrix\ny_pred = model_lstm.predict(X_test)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(y_test, axis=1)\nprint(\"\\nLSTM Classification Report:\")\nprint(classification_report(y_true_labels, y_pred_labels, target_names=le.classes_))\ncm = confusion_matrix(y_true_labels, y_pred_labels)\n\n# Plot Confusion Matrix (Counts)\nplot_confusion_matrix(cm, classes=le.classes_, title='LSTM Confusion Matrix (Counts)')\n\n# Plot Confusion Matrix (Normalized)\nplot_confusion_matrix(cm, classes=le.classes_, normalize=True, title='LSTM Normalized Confusion Matrix')\n\n# Plot Training & Validation Accuracy and Loss\nprint(\"\\n--- Plotting Training History ---\")\nplt.figure(figsize=(12, 5))\n\n# Plot Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history_lstm.history['accuracy'], label='Training Accuracy')\nplt.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Plot Loss\nplt.subplot(1, 2, 2)\nplt.plot(history_lstm.history['loss'], label='Training Loss')\nplt.plot(history_lstm.history['val_loss'], label='Validation Loss')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:10:33.494764Z","iopub.execute_input":"2025-09-02T16:10:33.495037Z","iopub.status.idle":"2025-09-02T16:11:54.634244Z","shell.execute_reply.started":"2025-09-02T16:10:33.495019Z","shell.execute_reply":"2025-09-02T16:11:54.633523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#----------------------------------------------\n# Experiment 2: CNN-LSTM Hybrid Model\n#----------------------------------------------\nprint(\"--- Starting CNN-LSTM Hybrid Model Experiment ---\")\n\ndef build_cnn_lstm_model(input_shape, num_classes):\n    # Input shape: (features, time, 1)\n    inp = layers.Input(shape=input_shape)\n\n    # CNN part for feature extraction\n    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inp)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D((2,2))(x)\n    \n    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    # Reshape CNN output to feed into LSTM\n    current_shape = x.shape\n    x = layers.Reshape((current_shape[2], current_shape[1] * current_shape[3]))(x)\n\n    # LSTM part for sequence analysis\n    x = layers.LSTM(64)(x)\n    x = layers.Dropout(0.4)(x)\n    \n    out = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = models.Model(inputs=inp, outputs=out)\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# Step 1: Load data and encode labels\nX, Y = load_dataset(DATA_PATH)\nle = LabelEncoder()\ny_encoded = le.fit_transform(Y)\nnum_classes = len(le.classes_)\ny_cat = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n\n# Step 2: Reshape data for CNN input (add channel dimension)\n# Shape change: (samples, features, time) -> (samples, features, time, 1)\nprint(\"Reshaping data for CNN-LSTM...\")\nX_for_cnn = X[..., np.newaxis]\nprint(\"New data shape:\", X_for_cnn.shape)\n\n# Step 3: Split data into Train-Test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_for_cnn, y_cat, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded\n)\n\n# Step 4: Build the CNN-LSTM model\ninput_shape_cnn_lstm = (X_train.shape[1], X_train.shape[2], X_train.shape[3]) # Shape: (features, time, 1)\nmodel_cnn_lstm = build_cnn_lstm_model(input_shape_cnn_lstm, num_classes)\nmodel_cnn_lstm.summary()\n\n# Define Callbacks\ncb = [\n    callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n    callbacks.ModelCheckpoint(\"ravdess_ser_cnn_lstm.h5\", save_best_only=True, monitor='val_loss')\n]\n\n# Step 5: Train the model\nprint(\"\\n--- Starting CNN-LSTM Model Training ---\")\nhistory_cnn_lstm = model_cnn_lstm.fit(\n    X_train, y_train,\n    validation_split=0.1,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=cb,\n    verbose=1\n)\n\n# Step 6: Evaluate the model\nprint(\"\\n--- CNN-LSTM Model Evaluation ---\")\nloss, acc = model_cnn_lstm.evaluate(X_test, y_test, verbose=0)\nprint(f\"CNN-LSTM Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n\ncnn_lstm_result = {'Model': 'CNN-LSTM', 'Test Accuracy': acc, 'Test Loss': loss}\n\n\n# Classification report and confusion matrix\ny_pred = model_cnn_lstm.predict(X_test)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(y_test, axis=1)\nprint(\"\\nCNN-LSTM Classification Report:\")\nprint(classification_report(y_true_labels, y_pred_labels, target_names=le.classes_))\ncm = confusion_matrix(y_true_labels, y_pred_labels)\n\n# Plot Confusion Matrix (Counts)\nplot_confusion_matrix(cm, classes=le.classes_, title='CNN-LSTM Confusion Matrix (Counts)')\n\n# Plot Confusion Matrix (Normalized)\nplot_confusion_matrix(cm, classes=le.classes_, normalize=True, title='CNN-LSTM Normalized Confusion Matrix')\n\n# Plot Training & Validation Accuracy and Loss\nprint(\"\\n--- Plotting Training History ---\")\nplt.figure(figsize=(12, 5))\n\n# Plot Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history_cnn_lstm.history['accuracy'], label='Training Accuracy')\nplt.plot(history_cnn_lstm.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Plot Loss\nplt.subplot(1, 2, 2)\nplt.plot(history_cnn_lstm.history['loss'], label='Training Loss')\nplt.plot(history_cnn_lstm.history['val_loss'], label='Validation Loss')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:12:12.366543Z","iopub.execute_input":"2025-09-02T16:12:12.367135Z","iopub.status.idle":"2025-09-02T16:14:59.728908Z","shell.execute_reply.started":"2025-09-02T16:12:12.367114Z","shell.execute_reply":"2025-09-02T16:14:59.728247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n#        Concise Final Comparison (Assuming Functions are Pre-defined)\n# ===================================================================\n\n# Step 1: Prepare data shapes and results list\nprint(\"Loading data once for all experiments...\")\nX, Y = load_dataset(DATA_PATH)\nle = LabelEncoder()\ny_encoded = le.fit_transform(Y)\nnum_classes = len(le.classes_)\ny_cat = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n\n# Prepare different data shapes required for the models\nX_cnn_shape = X[..., np.newaxis]\nX_lstm_shape = np.transpose(X, (0, 2, 1))\n\nresults = []\ncb = [callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n\n# ---------------------------\n# Step 2: Run Experiments Sequentially\n# ---------------------------\n\n# --- Experiment 1: CNN ---\nprint(\"\\n\\n--- Running CNN Model Experiment ---\")\nX_train, X_test, y_train, y_test = train_test_split(X_cnn_shape, y_cat, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded)\nmodel_cnn = build_cnn_model((X_train.shape[1], X_train.shape[2], 1), num_classes)\nmodel_cnn.fit(X_train, y_train, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cb, verbose=0)\nloss, acc = model_cnn.evaluate(X_test, y_test, verbose=0)\nresults.append({'Model': 'CNN', 'Test Accuracy': acc, 'Test Loss': loss})\nprint(\"CNN Model Evaluation Complete.\")\n\n# --- Experiment 2: LSTM ---\nprint(\"\\n--- Running LSTM Model Experiment ---\")\nX_train, X_test, y_train, y_test = train_test_split(X_lstm_shape, y_cat, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded)\nmodel_lstm = build_lstm_model((X_train.shape[1], X_train.shape[2]), num_classes)\nmodel_lstm.fit(X_train, y_train, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cb, verbose=0)\nloss, acc = model_lstm.evaluate(X_test, y_test, verbose=0)\nresults.append({'Model': 'LSTM', 'Test Accuracy': acc, 'Test Loss': loss})\nprint(\"LSTM Model Evaluation Complete.\")\n\n# --- Experiment 3: CNN-LSTM ---\nprint(\"\\n--- Running CNN-LSTM Model Experiment ---\")\nX_train, X_test, y_train, y_test = train_test_split(X_cnn_shape, y_cat, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded)\nmodel_cnn_lstm = build_cnn_lstm_model((X_train.shape[1], X_train.shape[2], 1), num_classes)\nmodel_cnn_lstm.fit(X_train, y_train, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cb, verbose=0)\nloss, acc = model_cnn_lstm.evaluate(X_test, y_test, verbose=0)\nresults.append({'Model': 'CNN-LSTM', 'Test Accuracy': acc, 'Test Loss': loss})\nprint(\"CNN-LSTM Model Evaluation Complete.\")\n\n# ---------------------------\n# Step 3: Display Final Summary Table\n# ---------------------------\nprint(\"\\n\\n\" + \"=\"*50)\nprint(\"             FINAL MODEL COMPARISON\")\nprint(\"=\"*50)\n\ndf_results = pd.DataFrame(results)\ndf_results['Test Accuracy'] = df_results['Test Accuracy'].apply(lambda x: f\"{x:.4f}\")\ndf_results['Test Loss'] = df_results['Test Loss'].apply(lambda x: f\"{x:.4f}\")\ndf_results.sort_values(by='Test Accuracy', ascending=False, inplace=True)\ndf_results.set_index('Model', inplace=True)\n\nprint(df_results)\nprint(\"\\n--- Comparison Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:23:35.656819Z","iopub.execute_input":"2025-09-02T16:23:35.657093Z","iopub.status.idle":"2025-09-02T16:27:04.204335Z","shell.execute_reply.started":"2025-09-02T16:23:35.657073Z","shell.execute_reply":"2025-09-02T16:27:04.20352Z"}},"outputs":[],"execution_count":null}]}